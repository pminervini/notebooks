{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_triples(path):\n",
    "    triples = []\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f.readlines():\n",
    "            s, p, o = line.split()\n",
    "            triples += [(s.strip(), p.strip(), o.strip())]\n",
    "    return triples\n",
    "\n",
    "\n",
    "def unit_cube_projection(var_matrix):\n",
    "    unit_cube_projection = tf.minimum(1., tf.maximum(var_matrix, 0.))\n",
    "    return tf.assign(var_matrix, unit_cube_projection)\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size / float(batch_size)))\n",
    "    res = [(i * batch_size, min(size, (i + 1) * batch_size)) for i in range(0, nb_batch)]\n",
    "    return res\n",
    "\n",
    "class IndexGenerator:\n",
    "    def __init__(self):\n",
    "        self.random_state = np.random.RandomState(0)\n",
    "\n",
    "    def __call__(self, n_samples, candidate_indices):\n",
    "        shuffled_indices = candidate_indices[self.random_state.permutation(len(candidate_indices))]\n",
    "        rand_ints = shuffled_indices[np.arange(n_samples) % len(shuffled_indices)]\n",
    "        return rand_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistMult:\n",
    "    def __init__(self, subject_embeddings=None, object_embeddings=None,\n",
    "                 predicate_embeddings=None,):\n",
    "        self.subject_embeddings, self.object_embeddings = subject_embeddings, object_embeddings\n",
    "        self.predicate_embeddings = predicate_embeddings\n",
    "\n",
    "    def __call__(self):\n",
    "        scores = tf.reduce_sum(self.subject_embeddings *\n",
    "                               self.predicate_embeddings *\n",
    "                               self.object_embeddings, axis=1)\n",
    "        return scores\n",
    "\n",
    "class ComplEx:\n",
    "    def __init__(self, subject_embeddings=None, object_embeddings=None,\n",
    "                 predicate_embeddings=None,):\n",
    "        self.subject_embeddings, self.object_embeddings = subject_embeddings, object_embeddings\n",
    "        self.predicate_embeddings = predicate_embeddings\n",
    "\n",
    "    def __call__(self):\n",
    "        es_re, es_im = tf.split(value=self.subject_embeddings, num_or_size_splits=2, axis=1)\n",
    "        eo_re, eo_im = tf.split(value=self.object_embeddings, num_or_size_splits=2, axis=1)\n",
    "        ew_re, ew_im = tf.split(value=self.predicate_embeddings, num_or_size_splits=2, axis=1)\n",
    "\n",
    "        def dot3(arg1, rel, arg2):\n",
    "            return tf.reduce_sum(arg1 * rel * arg2, axis=1)\n",
    "\n",
    "        scores = dot3(es_re, ew_re, eo_re) + dot3(es_re, ew_im, eo_im) + dot3(es_im, ew_re, eo_im) - dot3(es_im, ew_im, eo_re)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_embedding_size = 150\n",
    "predicate_embedding_size = 150\n",
    "\n",
    "seed = 0\n",
    "margin = 5\n",
    "\n",
    "nb_epochs = 100\n",
    "\n",
    "nb_discriminator_epochs = 1\n",
    "nb_adversary_epochs = 10\n",
    "\n",
    "nb_batches = 10\n",
    "\n",
    "violation_loss_weight = 0.1\n",
    "adversary_batch_size = 5\n",
    "\n",
    "np.random.seed(seed)\n",
    "random_state = np.random.RandomState(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "dataset_name = 'fb122'\n",
    "\n",
    "train_triples = read_triples('{}/{}.train.tsv'.format(dataset_name, dataset_name))\n",
    "valid_triples = read_triples('{}/{}.valid.tsv'.format(dataset_name, dataset_name))\n",
    "test_triples = read_triples('{}/{}.test.tsv'.format(dataset_name, dataset_name))\n",
    "\n",
    "from parse import parse_clause\n",
    "with open('{}/{}-clauses.pl'.format(dataset_name, dataset_name), 'rt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "clauses = [parse_clause(line.strip()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_triples = train_triples + valid_triples + test_triples\n",
    "\n",
    "entity_set = {s for (s, p, o) in all_triples} | {o for (s, p, o) in all_triples}\n",
    "predicate_set = {p for (s, p, o) in all_triples}\n",
    "\n",
    "nb_entities, nb_predicates = len(entity_set), len(predicate_set)\n",
    "nb_examples = len(train_triples)\n",
    "\n",
    "entity_to_idx = {entity: idx for idx, entity in enumerate(sorted(entity_set))}\n",
    "predicate_to_idx = {predicate: idx for idx, predicate in enumerate(sorted(predicate_set))}\n",
    "\n",
    "entity_embedding_layer = tf.get_variable('entities', shape=[nb_entities, entity_embedding_size],\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "predicate_embedding_layer = tf.get_variable('predicates', shape=[nb_predicates, predicate_embedding_size],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "subject_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "predicate_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "object_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "target_inputs = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "subject_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, subject_inputs)\n",
    "predicate_embeddings = tf.nn.embedding_lookup(predicate_embedding_layer, predicate_inputs)\n",
    "object_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, object_inputs)\n",
    "\n",
    "model_parameters = {\n",
    "    'subject_embeddings': subject_embeddings,\n",
    "    'predicate_embeddings': predicate_embeddings,\n",
    "    'object_embeddings': object_embeddings\n",
    "}\n",
    "\n",
    "model_class = ComplEx\n",
    "model = model_class(**model_parameters)\n",
    "\n",
    "scores = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adversary:\n",
    "    \"\"\"\n",
    "    Utility class for, given a set of clauses, computing the symbolic violation loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clauses, predicate_to_index,\n",
    "                 entity_embedding_layer, predicate_embedding_layer,\n",
    "                 model_class, model_parameters, loss_margin=0.0, batch_size=1):\n",
    "\n",
    "        self.clauses, self.predicate_to_index = clauses, predicate_to_index\n",
    "        self.entity_embedding_layer = entity_embedding_layer\n",
    "        self.predicate_embedding_layer = predicate_embedding_layer\n",
    "\n",
    "        self.entity_embedding_size = self.entity_embedding_layer.get_shape()[-1].value\n",
    "\n",
    "        self.model_class, self.model_parameters = model_class, model_parameters\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        def _violation_losses(body_scores, head_scores, margin):\n",
    "            _losses = tf.nn.relu(margin - head_scores + body_scores)\n",
    "            return tf.reduce_max(_losses)\n",
    "\n",
    "        self.loss_function = lambda body_scores, head_scores:\\\n",
    "            _violation_losses(body_scores, head_scores, margin=loss_margin)\n",
    "\n",
    "        # Symbolic functions computing the continuous loss\n",
    "        self.loss = tf.constant(.0)\n",
    "\n",
    "        # Trainable parameters of the adversarial model\n",
    "        self.parameters = []\n",
    "\n",
    "        # Mapping {clause:v2l} where \"clause\" is a clause, and v2l is a {var_name:layer} mapping\n",
    "        self.clause_to_variable_name_to_layer = dict()\n",
    "        self.clause_to_loss = dict()\n",
    "\n",
    "        for clause_idx, clause in enumerate(clauses):\n",
    "            clause_loss, clause_parameters, variable_name_to_layer =\\\n",
    "                self._parse_clause('clause_{}'.format(clause_idx), clause)\n",
    "\n",
    "            self.clause_to_variable_name_to_layer[clause] = variable_name_to_layer\n",
    "            self.clause_to_loss[clause] = clause_loss\n",
    "\n",
    "            self.loss += clause_loss\n",
    "            self.parameters += clause_parameters\n",
    "\n",
    "    def _parse_atom(self, atom, variable_name_to_layer):\n",
    "        \"\"\"\n",
    "        Given an atom in the form p(X, Y), where X and Y are associated to two distinct [1, k] embedding layers,\n",
    "        return the symbolic score of the atom.\n",
    "        \"\"\"\n",
    "        predicate_idx = self.predicate_to_index[atom.predicate.name]\n",
    "        \n",
    "        # [batch_size x 1 x embedding_size] tensor\n",
    "        predicate_embeddings = tf.nn.embedding_lookup(self.predicate_embedding_layer, [predicate_idx] * self.batch_size)\n",
    "        arg1_name, arg2_name = atom.arguments[0].name, atom.arguments[1].name\n",
    "\n",
    "        # [batch_size x embedding_size] variables\n",
    "        arg1_layer, arg2_layer = variable_name_to_layer[arg1_name], variable_name_to_layer[arg2_name]\n",
    "\n",
    "        subject_embeddings = variable_name_to_layer[arg1_name]\n",
    "        object_embeddings = variable_name_to_layer[arg2_name]\n",
    "\n",
    "        model_parameters = self.model_parameters\n",
    "        \n",
    "        model_parameters['subject_embeddings'] = subject_embeddings\n",
    "        model_parameters['object_embeddings'] = object_embeddings\n",
    "        \n",
    "        model_parameters['predicate_embeddings'] = predicate_embeddings\n",
    "\n",
    "        scoring_model = self.model_class(**model_parameters)\n",
    "        atom_score = scoring_model()\n",
    "\n",
    "        return atom_score\n",
    "\n",
    "    def _parse_conjunction(self, atoms, variable_name_to_layer):\n",
    "        \"\"\"\n",
    "        Given a conjunction of atoms in the form p(X0, X1), q(X2, X3), r(X4, X5), return its symbolic score.\n",
    "        \"\"\"\n",
    "        conjunction_score = None\n",
    "        for atom in atoms:\n",
    "            atom_score = self._parse_atom(atom, variable_name_to_layer=variable_name_to_layer)\n",
    "            conjunction_score = atom_score if conjunction_score is None else tf.minimum(conjunction_score, atom_score)\n",
    "        return conjunction_score\n",
    "\n",
    "    def _parse_clause(self, name, clause):\n",
    "        \"\"\"\n",
    "        Given a clause in the form p(X0, X1) :- q(X2, X3), r(X4, X5), return its symbolic score.\n",
    "        \"\"\"\n",
    "        head, body = clause.head, clause.body\n",
    "\n",
    "        # Enumerate all variables\n",
    "        variable_names = {argument.name for argument in head.arguments}\n",
    "        for body_atom in body:\n",
    "            variable_names |= {argument.name for argument in body_atom.arguments}\n",
    "\n",
    "        # Instantiate a new layer for each variable\n",
    "        variable_name_to_layer = dict()\n",
    "        for variable_name in sorted(variable_names):\n",
    "            # [batch_size, embedding_size] variable\n",
    "            variable_layer = tf.get_variable('{}_{}_violator'.format(name, variable_name),\n",
    "                                             shape=[self.batch_size, self.entity_embedding_size],\n",
    "                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            variable_name_to_layer[variable_name] = variable_layer\n",
    "\n",
    "        head_score = self._parse_atom(head, variable_name_to_layer=variable_name_to_layer)\n",
    "        body_score = self._parse_conjunction(body, variable_name_to_layer=variable_name_to_layer)\n",
    "\n",
    "        parameters = [variable_name_to_layer[variable_name] for variable_name in sorted(variable_names)]\n",
    "        loss = self.loss_function(body_score, head_score)\n",
    "        return loss, parameters, variable_name_to_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adversary = Adversary(clauses=clauses, predicate_to_index=predicate_to_idx,\n",
    "                      entity_embedding_layer=entity_embedding_layer,\n",
    "                      predicate_embedding_layer=predicate_embedding_layer,\n",
    "                      model_class=model_class, model_parameters=model_parameters,\n",
    "                      batch_size=adversary_batch_size)\n",
    "\n",
    "adversary_init_op = tf.variables_initializer(var_list=adversary.parameters, name='init_adversary')\n",
    "violation_loss = adversary.loss\n",
    "\n",
    "ADVERSARIAL_OPTIMIZER_SCOPE_NAME = 'adversary/optimizer'\n",
    "with tf.variable_scope(ADVERSARIAL_OPTIMIZER_SCOPE_NAME):\n",
    "    adversarial_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    adversarial_training_step = adversarial_optimizer.minimize(- violation_loss, var_list=adversary.parameters)\n",
    "\n",
    "adversary_optimizer_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=ADVERSARIAL_OPTIMIZER_SCOPE_NAME)\n",
    "adversary_optimizer_vars_init_op = tf.variables_initializer(adversary_optimizer_vars)\n",
    "\n",
    "adversary_projections = [unit_cube_projection(emb) for emb in adversary.parameters]\n",
    "\n",
    "\n",
    "hinge_losses = tf.nn.relu(margin - scores * (2 * target_inputs - 1))\n",
    "\n",
    "loss = tf.reduce_sum(hinge_losses) + violation_loss_weight * violation_loss\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "training_step = optimizer.minimize(loss, var_list=[entity_embedding_layer, predicate_embedding_layer])\n",
    "\n",
    "projection_step = unit_cube_projection(entity_embedding_layer)\n",
    "\n",
    "\n",
    "import math\n",
    "batch_size = math.ceil(nb_examples / nb_batches)\n",
    "batches = make_batches(nb_examples, batch_size)\n",
    "\n",
    "nb_versions = 3\n",
    "\n",
    "Xs = np.array([entity_to_idx[s] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xp = np.array([predicate_to_idx[p] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xo = np.array([entity_to_idx[o] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "\n",
    "index_gen = IndexGenerator()\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 1/1\tLoss value: 14.9996 ± 0.0005\n",
      "INFO:__main__:Epoch 1/1\tLoss value: 0.037005141377449036\n",
      "INFO:__main__:Epoch 1/2\tLoss value: 0.05832251161336899\n",
      "INFO:__main__:Epoch 1/3\tLoss value: 0.08015210926532745\n",
      "INFO:__main__:Epoch 1/4\tLoss value: 0.10290764272212982\n",
      "INFO:__main__:Epoch 1/5\tLoss value: 0.12751871347427368\n",
      "INFO:__main__:Epoch 1/6\tLoss value: 0.15434563159942627\n",
      "INFO:__main__:Epoch 1/7\tLoss value: 0.18477438390254974\n",
      "INFO:__main__:Epoch 1/8\tLoss value: 0.2192409336566925\n",
      "INFO:__main__:Epoch 1/9\tLoss value: 0.25854042172431946\n",
      "INFO:__main__:Epoch 1/10\tLoss value: 0.3039083778858185\n",
      "INFO:__main__:Epoch 2/1\tLoss value: 14.6677 ± 0.4112\n",
      "INFO:__main__:Epoch 2/1\tLoss value: 27.937274932861328\n",
      "INFO:__main__:Epoch 2/2\tLoss value: 47.388267517089844\n",
      "INFO:__main__:Epoch 2/3\tLoss value: 69.58460235595703\n",
      "INFO:__main__:Epoch 2/4\tLoss value: 95.48681640625\n",
      "INFO:__main__:Epoch 2/5\tLoss value: 125.84796142578125\n",
      "INFO:__main__:Epoch 2/6\tLoss value: 160.33551025390625\n",
      "INFO:__main__:Epoch 2/7\tLoss value: 199.41590881347656\n",
      "INFO:__main__:Epoch 2/8\tLoss value: 242.7416229248047\n",
      "INFO:__main__:Epoch 2/9\tLoss value: 290.0210876464844\n",
      "INFO:__main__:Epoch 2/10\tLoss value: 341.21929931640625\n",
      "INFO:__main__:Epoch 3/1\tLoss value: 10.5054 ± 1.8336\n",
      "INFO:__main__:Epoch 3/1\tLoss value: 67.00949096679688\n",
      "INFO:__main__:Epoch 3/2\tLoss value: 138.52745056152344\n",
      "INFO:__main__:Epoch 3/3\tLoss value: 212.14288330078125\n",
      "INFO:__main__:Epoch 3/4\tLoss value: 289.3619384765625\n",
      "INFO:__main__:Epoch 3/5\tLoss value: 374.4559631347656\n",
      "INFO:__main__:Epoch 3/6\tLoss value: 464.1065368652344\n",
      "INFO:__main__:Epoch 3/7\tLoss value: 558.7449951171875\n",
      "INFO:__main__:Epoch 3/8\tLoss value: 658.7227783203125\n",
      "INFO:__main__:Epoch 3/9\tLoss value: 761.2094116210938\n",
      "INFO:__main__:Epoch 3/10\tLoss value: 865.8599243164062\n",
      "INFO:__main__:Epoch 4/1\tLoss value: 6.5857 ± 0.3513\n",
      "INFO:__main__:Epoch 4/1\tLoss value: 54.379486083984375\n",
      "INFO:__main__:Epoch 4/2\tLoss value: 147.379150390625\n",
      "INFO:__main__:Epoch 4/3\tLoss value: 244.63243103027344\n",
      "INFO:__main__:Epoch 4/4\tLoss value: 336.07159423828125\n",
      "INFO:__main__:Epoch 4/5\tLoss value: 436.36920166015625\n",
      "INFO:__main__:Epoch 4/6\tLoss value: 540.1325073242188\n",
      "INFO:__main__:Epoch 4/7\tLoss value: 646.1339111328125\n",
      "INFO:__main__:Epoch 4/8\tLoss value: 754.5147094726562\n",
      "INFO:__main__:Epoch 4/9\tLoss value: 864.042236328125\n",
      "INFO:__main__:Epoch 4/10\tLoss value: 971.8656616210938\n",
      "INFO:__main__:Epoch 5/1\tLoss value: 5.3929 ± 0.1513\n",
      "INFO:__main__:Epoch 5/1\tLoss value: 60.66802978515625\n",
      "INFO:__main__:Epoch 5/2\tLoss value: 169.662841796875\n",
      "INFO:__main__:Epoch 5/3\tLoss value: 279.13446044921875\n",
      "INFO:__main__:Epoch 5/4\tLoss value: 387.0870361328125\n",
      "INFO:__main__:Epoch 5/5\tLoss value: 504.5575256347656\n",
      "INFO:__main__:Epoch 5/6\tLoss value: 622.1484375\n",
      "INFO:__main__:Epoch 5/7\tLoss value: 745.43359375\n",
      "INFO:__main__:Epoch 5/8\tLoss value: 866.3714599609375\n",
      "INFO:__main__:Epoch 5/9\tLoss value: 987.5112915039062\n",
      "INFO:__main__:Epoch 5/10\tLoss value: 1104.11474609375\n",
      "INFO:__main__:Epoch 6/1\tLoss value: 4.5742 ± 0.0966\n",
      "INFO:__main__:Epoch 6/1\tLoss value: 71.15213012695312\n",
      "INFO:__main__:Epoch 6/2\tLoss value: 188.0497589111328\n",
      "INFO:__main__:Epoch 6/3\tLoss value: 307.7999572753906\n",
      "INFO:__main__:Epoch 6/4\tLoss value: 421.56268310546875\n",
      "INFO:__main__:Epoch 6/5\tLoss value: 538.3877563476562\n",
      "INFO:__main__:Epoch 6/6\tLoss value: 658.9730834960938\n",
      "INFO:__main__:Epoch 6/7\tLoss value: 780.6898193359375\n",
      "INFO:__main__:Epoch 6/8\tLoss value: 903.0748291015625\n",
      "INFO:__main__:Epoch 6/9\tLoss value: 1023.8822021484375\n",
      "INFO:__main__:Epoch 6/10\tLoss value: 1139.8067626953125\n",
      "INFO:__main__:Epoch 7/1\tLoss value: 3.9783 ± 0.0802\n",
      "INFO:__main__:Epoch 7/1\tLoss value: 108.32939147949219\n",
      "INFO:__main__:Epoch 7/2\tLoss value: 259.8048095703125\n",
      "INFO:__main__:Epoch 7/3\tLoss value: 397.495361328125\n",
      "INFO:__main__:Epoch 7/4\tLoss value: 530.4791870117188\n",
      "INFO:__main__:Epoch 7/5\tLoss value: 662.435546875\n",
      "INFO:__main__:Epoch 7/6\tLoss value: 797.0531616210938\n",
      "INFO:__main__:Epoch 7/7\tLoss value: 930.1895751953125\n",
      "INFO:__main__:Epoch 7/8\tLoss value: 1056.8377685546875\n",
      "INFO:__main__:Epoch 7/9\tLoss value: 1186.31103515625\n",
      "INFO:__main__:Epoch 7/10\tLoss value: 1302.7999267578125\n",
      "INFO:__main__:Epoch 8/1\tLoss value: 3.5289 ± 0.0313\n",
      "INFO:__main__:Epoch 8/1\tLoss value: 86.37255859375\n",
      "INFO:__main__:Epoch 8/2\tLoss value: 245.46218872070312\n",
      "INFO:__main__:Epoch 8/3\tLoss value: 376.4599609375\n",
      "INFO:__main__:Epoch 8/4\tLoss value: 517.5479125976562\n",
      "INFO:__main__:Epoch 8/5\tLoss value: 639.9364624023438\n",
      "INFO:__main__:Epoch 8/6\tLoss value: 774.43798828125\n",
      "INFO:__main__:Epoch 8/7\tLoss value: 899.12744140625\n",
      "INFO:__main__:Epoch 8/8\tLoss value: 1023.1798706054688\n",
      "INFO:__main__:Epoch 8/9\tLoss value: 1141.3125\n",
      "INFO:__main__:Epoch 8/10\tLoss value: 1254.4583740234375\n",
      "INFO:__main__:Epoch 9/1\tLoss value: 3.1341 ± 0.0297\n",
      "INFO:__main__:Epoch 9/1\tLoss value: 78.77142333984375\n",
      "INFO:__main__:Epoch 9/2\tLoss value: 216.1274871826172\n",
      "INFO:__main__:Epoch 9/3\tLoss value: 350.9869384765625\n",
      "INFO:__main__:Epoch 9/4\tLoss value: 471.5009460449219\n",
      "INFO:__main__:Epoch 9/5\tLoss value: 595.7521362304688\n",
      "INFO:__main__:Epoch 9/6\tLoss value: 713.3211059570312\n",
      "INFO:__main__:Epoch 9/7\tLoss value: 836.366943359375\n",
      "INFO:__main__:Epoch 9/8\tLoss value: 949.6336059570312\n",
      "INFO:__main__:Epoch 9/9\tLoss value: 1064.1300048828125\n",
      "INFO:__main__:Epoch 9/10\tLoss value: 1170.78466796875\n",
      "INFO:__main__:Epoch 10/1\tLoss value: 2.8532 ± 0.0499\n",
      "INFO:__main__:Epoch 10/1\tLoss value: 95.59754943847656\n",
      "INFO:__main__:Epoch 10/2\tLoss value: 256.1995544433594\n",
      "INFO:__main__:Epoch 10/3\tLoss value: 415.345458984375\n",
      "INFO:__main__:Epoch 10/4\tLoss value: 557.3989868164062\n",
      "INFO:__main__:Epoch 10/5\tLoss value: 700.972412109375\n",
      "INFO:__main__:Epoch 10/6\tLoss value: 838.5853881835938\n",
      "INFO:__main__:Epoch 10/7\tLoss value: 982.8052368164062\n",
      "INFO:__main__:Epoch 10/8\tLoss value: 1111.483154296875\n",
      "INFO:__main__:Epoch 10/9\tLoss value: 1246.389404296875\n",
      "INFO:__main__:Epoch 10/10\tLoss value: 1363.168701171875\n",
      "INFO:__main__:Epoch 11/1\tLoss value: 2.5844 ± 0.0384\n",
      "INFO:__main__:Epoch 11/1\tLoss value: 90.58894348144531\n",
      "INFO:__main__:Epoch 11/2\tLoss value: 234.47836303710938\n",
      "INFO:__main__:Epoch 11/3\tLoss value: 364.4402160644531\n",
      "INFO:__main__:Epoch 11/4\tLoss value: 488.31915283203125\n",
      "INFO:__main__:Epoch 11/5\tLoss value: 608.6617431640625\n",
      "INFO:__main__:Epoch 11/6\tLoss value: 731.4077758789062\n",
      "INFO:__main__:Epoch 11/7\tLoss value: 844.6109619140625\n",
      "INFO:__main__:Epoch 11/8\tLoss value: 959.2591552734375\n",
      "INFO:__main__:Epoch 11/9\tLoss value: 1067.35546875\n",
      "INFO:__main__:Epoch 11/10\tLoss value: 1168.385986328125\n",
      "INFO:__main__:Epoch 12/1\tLoss value: 2.3979 ± 0.0434\n",
      "INFO:__main__:Epoch 12/1\tLoss value: 79.91600036621094\n",
      "INFO:__main__:Epoch 12/2\tLoss value: 230.9688720703125\n",
      "INFO:__main__:Epoch 12/3\tLoss value: 385.2007751464844\n",
      "INFO:__main__:Epoch 12/4\tLoss value: 507.870849609375\n",
      "INFO:__main__:Epoch 12/5\tLoss value: 643.5989990234375\n",
      "INFO:__main__:Epoch 12/6\tLoss value: 768.223388671875\n",
      "INFO:__main__:Epoch 12/7\tLoss value: 894.1620483398438\n",
      "INFO:__main__:Epoch 12/8\tLoss value: 1019.3241577148438\n",
      "INFO:__main__:Epoch 12/9\tLoss value: 1133.106689453125\n",
      "INFO:__main__:Epoch 12/10\tLoss value: 1245.9158935546875\n",
      "INFO:__main__:Epoch 13/1\tLoss value: 2.2047 ± 0.0246\n",
      "INFO:__main__:Epoch 13/1\tLoss value: 94.26434326171875\n",
      "INFO:__main__:Epoch 13/2\tLoss value: 258.0623474121094\n",
      "INFO:__main__:Epoch 13/3\tLoss value: 395.2290954589844\n",
      "INFO:__main__:Epoch 13/4\tLoss value: 526.0809326171875\n",
      "INFO:__main__:Epoch 13/5\tLoss value: 646.218994140625\n",
      "INFO:__main__:Epoch 13/6\tLoss value: 766.4026489257812\n",
      "INFO:__main__:Epoch 13/7\tLoss value: 877.7215576171875\n",
      "INFO:__main__:Epoch 13/8\tLoss value: 989.1353759765625\n",
      "INFO:__main__:Epoch 13/9\tLoss value: 1094.05029296875\n",
      "INFO:__main__:Epoch 13/10\tLoss value: 1191.3203125\n",
      "INFO:__main__:Epoch 14/1\tLoss value: 2.0813 ± 0.0380\n",
      "INFO:__main__:Epoch 14/1\tLoss value: 90.2054672241211\n",
      "INFO:__main__:Epoch 14/2\tLoss value: 222.7648468017578\n",
      "INFO:__main__:Epoch 14/3\tLoss value: 355.99237060546875\n",
      "INFO:__main__:Epoch 14/4\tLoss value: 470.3455505371094\n",
      "INFO:__main__:Epoch 14/5\tLoss value: 572.9320068359375\n",
      "INFO:__main__:Epoch 14/6\tLoss value: 682.7808227539062\n",
      "INFO:__main__:Epoch 14/7\tLoss value: 788.0059814453125\n",
      "INFO:__main__:Epoch 14/8\tLoss value: 885.8945922851562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 14/9\tLoss value: 980.7346801757812\n",
      "INFO:__main__:Epoch 14/10\tLoss value: 1075.2105712890625\n",
      "INFO:__main__:Epoch 15/1\tLoss value: 1.9502 ± 0.0283\n",
      "INFO:__main__:Epoch 15/1\tLoss value: 130.24859619140625\n",
      "INFO:__main__:Epoch 15/2\tLoss value: 319.31378173828125\n",
      "INFO:__main__:Epoch 15/3\tLoss value: 509.33135986328125\n",
      "INFO:__main__:Epoch 15/4\tLoss value: 665.1360473632812\n",
      "INFO:__main__:Epoch 15/5\tLoss value: 823.8536376953125\n",
      "INFO:__main__:Epoch 15/6\tLoss value: 970.2908325195312\n",
      "INFO:__main__:Epoch 15/7\tLoss value: 1117.1494140625\n",
      "INFO:__main__:Epoch 15/8\tLoss value: 1252.92529296875\n",
      "INFO:__main__:Epoch 15/9\tLoss value: 1378.828125\n",
      "INFO:__main__:Epoch 15/10\tLoss value: 1501.7156982421875\n",
      "INFO:__main__:Epoch 16/1\tLoss value: 1.8700 ± 0.0436\n",
      "INFO:__main__:Epoch 16/1\tLoss value: 99.4839096069336\n",
      "INFO:__main__:Epoch 16/2\tLoss value: 286.62646484375\n",
      "INFO:__main__:Epoch 16/3\tLoss value: 464.12646484375\n",
      "INFO:__main__:Epoch 16/4\tLoss value: 617.6766967773438\n",
      "INFO:__main__:Epoch 16/5\tLoss value: 768.6605224609375\n",
      "INFO:__main__:Epoch 16/6\tLoss value: 906.9293823242188\n",
      "INFO:__main__:Epoch 16/7\tLoss value: 1050.640380859375\n",
      "INFO:__main__:Epoch 16/8\tLoss value: 1181.0064697265625\n",
      "INFO:__main__:Epoch 16/9\tLoss value: 1306.41162109375\n",
      "INFO:__main__:Epoch 16/10\tLoss value: 1426.395751953125\n",
      "INFO:__main__:Epoch 17/1\tLoss value: 1.7621 ± 0.0157\n",
      "INFO:__main__:Epoch 17/1\tLoss value: 92.185302734375\n",
      "INFO:__main__:Epoch 17/2\tLoss value: 284.5326232910156\n",
      "INFO:__main__:Epoch 17/3\tLoss value: 465.41357421875\n",
      "INFO:__main__:Epoch 17/4\tLoss value: 621.428955078125\n",
      "INFO:__main__:Epoch 17/5\tLoss value: 773.66845703125\n",
      "INFO:__main__:Epoch 17/6\tLoss value: 921.8056030273438\n",
      "INFO:__main__:Epoch 17/7\tLoss value: 1065.903076171875\n",
      "INFO:__main__:Epoch 17/8\tLoss value: 1199.7479248046875\n",
      "INFO:__main__:Epoch 17/9\tLoss value: 1330.0518798828125\n",
      "INFO:__main__:Epoch 17/10\tLoss value: 1454.81201171875\n",
      "INFO:__main__:Epoch 18/1\tLoss value: 1.6942 ± 0.0346\n",
      "INFO:__main__:Epoch 18/1\tLoss value: 111.75216674804688\n",
      "INFO:__main__:Epoch 18/2\tLoss value: 289.0792236328125\n",
      "INFO:__main__:Epoch 18/3\tLoss value: 467.99346923828125\n",
      "INFO:__main__:Epoch 18/4\tLoss value: 613.46142578125\n",
      "INFO:__main__:Epoch 18/5\tLoss value: 753.604736328125\n",
      "INFO:__main__:Epoch 18/6\tLoss value: 894.119384765625\n",
      "INFO:__main__:Epoch 18/7\tLoss value: 1022.2001953125\n",
      "INFO:__main__:Epoch 18/8\tLoss value: 1147.1729736328125\n",
      "INFO:__main__:Epoch 18/9\tLoss value: 1266.08642578125\n",
      "INFO:__main__:Epoch 18/10\tLoss value: 1368.8175048828125\n"
     ]
    }
   ],
   "source": [
    "def stats(values):\n",
    "    return '{0:.4f} ± {1:.4f}'.format(round(np.mean(values), 4), round(np.std(values), 4))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_op)\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "\n",
    "\n",
    "    for discriminator_epoch in range(1, nb_discriminator_epochs + 1):\n",
    "        order = random_state.permutation(nb_examples)\n",
    "        Xs_shuf, Xp_shuf, Xo_shuf = Xs[order], Xp[order], Xo[order]\n",
    "\n",
    "        loss_values = []\n",
    "\n",
    "        for batch_no, (batch_start, batch_end) in enumerate(batches):\n",
    "            curr_batch_size = batch_end - batch_start\n",
    "\n",
    "            Xs_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xs_shuf.dtype)\n",
    "            Xp_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xp_shuf.dtype)\n",
    "            Xo_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xo_shuf.dtype)\n",
    "\n",
    "            Xs_batch[0::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "            Xp_batch[0::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "            Xo_batch[0::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "            # Xs_batch[1::nb_versions] needs to be corrupted\n",
    "            Xs_batch[1::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "            Xp_batch[1::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "            Xo_batch[1::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "            # Xo_batch[2::nb_versions] needs to be corrupted\n",
    "            Xs_batch[2::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "            Xp_batch[2::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "            Xo_batch[2::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "\n",
    "            feed_dict = {\n",
    "                subject_inputs: Xs_batch, predicate_inputs: Xp_batch, object_inputs: Xo_batch,\n",
    "                target_inputs: np.array([1.0, 0.0, 0.0] * curr_batch_size)\n",
    "            }\n",
    "\n",
    "            _, loss_value = session.run([training_step, loss], feed_dict=feed_dict)\n",
    "            session.run(projection_step)\n",
    "\n",
    "            loss_values += [loss_value / (Xp_batch.shape[0] / nb_versions)]\n",
    "\n",
    "        logger.info('Epoch {0}/{1}\\tLoss value: {2}'.format(epoch, discriminator_epoch, stats(loss_values)))\n",
    "\n",
    "\n",
    "    session.run([adversary_init_op, adversary_optimizer_vars_init_op])\n",
    "    entity_indices = np.array(sorted(entity_to_idx.values()))\n",
    "\n",
    "    def ground_init_op(adversarial_embeddings):\n",
    "        rnd_entity_indices = entity_indices[\n",
    "            random_state.randint(low=0,\n",
    "                                 high=len(entity_indices),\n",
    "                                 size=adversary_batch_size)]\n",
    "        entity_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, rnd_entity_indices)\n",
    "        return adversarial_embeddings.assign(entity_embeddings)\n",
    "\n",
    "    assignment_ops = [ground_init_op(emb) for emb in adversary.parameters]\n",
    "    session.run(assignment_ops)\n",
    "\n",
    "    for adversary_epoch in range(1, nb_adversary_epochs + 1):\n",
    "        _, violation_loss_value = session.run([adversarial_training_step, violation_loss])\n",
    "        logger.info('Epoch {0}/{1}\\tLoss value: {2}'.format(epoch, adversary_epoch, violation_loss_value))\n",
    "\n",
    "        session.run(adversary_projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for eval_name, eval_triples in [('valid', valid_triples), ('test', test_triples)]:\n",
    "\n",
    "    ranks_subj, ranks_obj = [], []\n",
    "    filtered_ranks_subj, filtered_ranks_obj = [], []\n",
    "\n",
    "    for _i, (s, p, o) in enumerate(eval_triples):\n",
    "        s_idx, p_idx, o_idx = entity_to_idx[s], predicate_to_idx[p], entity_to_idx[o]\n",
    "\n",
    "        Xs = np.full(shape=(nb_entities,), fill_value=s_idx, dtype=np.int32)\n",
    "        Xp = np.full(shape=(nb_entities,), fill_value=p_idx, dtype=np.int32)\n",
    "        Xo = np.full(shape=(nb_entities,), fill_value=o_idx, dtype=np.int32)\n",
    "\n",
    "        feed_dict_corrupt_subj = {subject_inputs: np.arange(nb_entities), predicate_inputs: Xp, object_inputs: Xo}\n",
    "        feed_dict_corrupt_obj = {subject_inputs: Xs, predicate_inputs: Xp, object_inputs: np.arange(nb_entities)}\n",
    "\n",
    "        # scores of (1, p, o), (2, p, o), .., (N, p, o)\n",
    "        scores_subj = session.run(scores, feed_dict=feed_dict_corrupt_subj)\n",
    "\n",
    "        # scores of (s, p, 1), (s, p, 2), .., (s, p, N)\n",
    "        scores_obj = session.run(scores, feed_dict=feed_dict_corrupt_obj)\n",
    "\n",
    "        ranks_subj += [1 + np.sum(scores_subj > scores_subj[s_idx])]\n",
    "        ranks_obj += [1 + np.sum(scores_obj > scores_obj[o_idx])]\n",
    "\n",
    "        filtered_scores_subj = scores_subj.copy()\n",
    "        filtered_scores_obj = scores_obj.copy()\n",
    "\n",
    "        rm_idx_s = [entity_to_idx[fs] for (fs, fp, fo) in all_triples if fs != s and fp == p and fo == o]\n",
    "        rm_idx_o = [entity_to_idx[fo] for (fs, fp, fo) in all_triples if fs == s and fp == p and fo != o]\n",
    "\n",
    "        filtered_scores_subj[rm_idx_s] = - np.inf\n",
    "        filtered_scores_obj[rm_idx_o] = - np.inf\n",
    "\n",
    "        filtered_ranks_subj += [1 + np.sum(filtered_scores_subj > filtered_scores_subj[s_idx])]\n",
    "        filtered_ranks_obj += [1 + np.sum(filtered_scores_obj > filtered_scores_obj[o_idx])]\n",
    "\n",
    "        if _i % 1000 == 0:\n",
    "            logger.info('{}/{} ..'.format(_i, len(eval_triples)))\n",
    "        \n",
    "        \n",
    "    ranks = ranks_subj + ranks_obj\n",
    "    filtered_ranks = filtered_ranks_subj + filtered_ranks_obj\n",
    "\n",
    "    for setting_name, setting_ranks in [('Raw', ranks), ('Filtered', filtered_ranks)]:\n",
    "        mean_rank = np.mean(setting_ranks)\n",
    "        logger.info('[{}] {} Mean Rank: {}'.format(eval_name, setting_name, mean_rank))\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n",
    "            logger.info('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
