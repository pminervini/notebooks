{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_triples(path):\n",
    "    triples = []\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f.readlines():\n",
    "            s, p, o = line.split()\n",
    "            triples += [(s.strip(), p.strip(), o.strip())]\n",
    "    return triples\n",
    "\n",
    "\n",
    "def unit_cube_projection(var_matrix):\n",
    "    unit_cube_projection = tf.minimum(1., tf.maximum(var_matrix, 0.))\n",
    "    return tf.assign(var_matrix, unit_cube_projection)\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size / float(batch_size)))\n",
    "    res = [(i * batch_size, min(size, (i + 1) * batch_size)) for i in range(0, nb_batch)]\n",
    "    return res\n",
    "\n",
    "class IndexGenerator:\n",
    "    def __init__(self):\n",
    "        self.random_state = np.random.RandomState(0)\n",
    "\n",
    "    def __call__(self, n_samples, candidate_indices):\n",
    "        shuffled_indices = candidate_indices[self.random_state.permutation(len(candidate_indices))]\n",
    "        rand_ints = shuffled_indices[np.arange(n_samples) % len(shuffled_indices)]\n",
    "        return rand_ints\n",
    "\n",
    "class DistMult:\n",
    "    def __init__(self, subject_embeddings=None, object_embeddings=None,\n",
    "                 predicate_embeddings=None,):\n",
    "        self.subject_embeddings, self.object_embeddings = subject_embeddings, object_embeddings\n",
    "        self.predicate_embeddings = predicate_embeddings\n",
    "\n",
    "    def __call__(self):\n",
    "        scores = tf.reduce_sum(self.subject_embeddings *\n",
    "                               self.predicate_embeddings *\n",
    "                               self.object_embeddings, axis=1)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_embedding_size = 150\n",
    "predicate_embedding_size = 150\n",
    "\n",
    "seed = 0\n",
    "margin = 5\n",
    "\n",
    "nb_epochs = 1000\n",
    "nb_batches = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "random_state = np.random.RandomState(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "dataset_name = 'wn18'\n",
    "\n",
    "train_triples = read_triples('{}/{}.train.tsv'.format(dataset_name, dataset_name))\n",
    "valid_triples = read_triples('{}/{}.valid.tsv'.format(dataset_name, dataset_name))\n",
    "test_triples = read_triples('{}/{}.test.tsv'.format(dataset_name, dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_triples = train_triples + valid_triples + test_triples\n",
    "\n",
    "entity_set = {s for (s, p, o) in all_triples} | {o for (s, p, o) in all_triples}\n",
    "predicate_set = {p for (s, p, o) in all_triples}\n",
    "\n",
    "nb_entities, nb_predicates = len(entity_set), len(predicate_set)\n",
    "nb_examples = len(train_triples)\n",
    "\n",
    "entity_to_idx = {entity: idx for idx, entity in enumerate(sorted(entity_set))}\n",
    "predicate_to_idx = {predicate: idx for idx, predicate in enumerate(sorted(predicate_set))}\n",
    "\n",
    "entity_embedding_layer = tf.get_variable('entities', shape=[nb_entities, entity_embedding_size],\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "predicate_embedding_layer = tf.get_variable('predicates', shape=[nb_predicates, predicate_embedding_size],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "subject_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "predicate_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "object_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "target_inputs = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "subject_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, subject_inputs)\n",
    "predicate_embeddings = tf.nn.embedding_lookup(predicate_embedding_layer, predicate_inputs)\n",
    "object_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, object_inputs)\n",
    "\n",
    "model = DistMult(subject_embeddings=subject_embeddings,\n",
    "                 predicate_embeddings=predicate_embeddings,\n",
    "                 object_embeddings=object_embeddings)\n",
    "\n",
    "scores = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "hinge_losses = tf.nn.relu(margin - scores * (2 * target_inputs - 1))\n",
    "loss = tf.reduce_sum(hinge_losses)\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "training_step = optimizer.minimize(loss)\n",
    "\n",
    "projection_step = unit_cube_projection(entity_embedding_layer)\n",
    "\n",
    "batch_size = math.ceil(nb_examples / nb_batches)\n",
    "batches = make_batches(nb_examples, batch_size)\n",
    "\n",
    "nb_versions = 3\n",
    "\n",
    "Xs = np.array([entity_to_idx[s] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xp = np.array([predicate_to_idx[p] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xo = np.array([entity_to_idx[o] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "\n",
    "index_gen = IndexGenerator()\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 10\tLoss value: 5.4755 ± 0.1380\n",
      "INFO:__main__:Epoch 20\tLoss value: 0.2603 ± 0.0061\n",
      "INFO:__main__:Epoch 30\tLoss value: 0.0730 ± 0.0038\n",
      "INFO:__main__:Epoch 40\tLoss value: 0.0492 ± 0.0039\n",
      "INFO:__main__:Epoch 50\tLoss value: 0.0430 ± 0.0032\n",
      "INFO:__main__:Epoch 60\tLoss value: 0.0363 ± 0.0037\n",
      "INFO:__main__:Epoch 70\tLoss value: 0.0326 ± 0.0031\n",
      "INFO:__main__:Epoch 80\tLoss value: 0.0317 ± 0.0043\n",
      "INFO:__main__:Epoch 90\tLoss value: 0.0275 ± 0.0033\n",
      "INFO:__main__:Epoch 100\tLoss value: 0.0259 ± 0.0034\n",
      "INFO:__main__:Epoch 110\tLoss value: 0.0246 ± 0.0043\n",
      "INFO:__main__:Epoch 120\tLoss value: 0.0243 ± 0.0044\n",
      "INFO:__main__:Epoch 130\tLoss value: 0.0214 ± 0.0025\n",
      "INFO:__main__:Epoch 140\tLoss value: 0.0233 ± 0.0037\n",
      "INFO:__main__:Epoch 150\tLoss value: 0.0218 ± 0.0032\n",
      "INFO:__main__:Epoch 160\tLoss value: 0.0204 ± 0.0027\n",
      "INFO:__main__:Epoch 170\tLoss value: 0.0197 ± 0.0030\n",
      "INFO:__main__:Epoch 180\tLoss value: 0.0201 ± 0.0037\n",
      "INFO:__main__:Epoch 190\tLoss value: 0.0200 ± 0.0036\n",
      "INFO:__main__:Epoch 200\tLoss value: 0.0210 ± 0.0027\n",
      "INFO:__main__:Epoch 210\tLoss value: 0.0206 ± 0.0034\n",
      "INFO:__main__:Epoch 220\tLoss value: 0.0195 ± 0.0036\n",
      "INFO:__main__:Epoch 230\tLoss value: 0.0212 ± 0.0028\n",
      "INFO:__main__:Epoch 240\tLoss value: 0.0188 ± 0.0037\n",
      "INFO:__main__:Epoch 250\tLoss value: 0.0214 ± 0.0032\n",
      "INFO:__main__:Epoch 260\tLoss value: 0.0199 ± 0.0033\n",
      "INFO:__main__:Epoch 270\tLoss value: 0.0172 ± 0.0031\n",
      "INFO:__main__:Epoch 280\tLoss value: 0.0166 ± 0.0020\n",
      "INFO:__main__:Epoch 290\tLoss value: 0.0177 ± 0.0035\n",
      "INFO:__main__:Epoch 300\tLoss value: 0.0197 ± 0.0032\n",
      "INFO:__main__:Epoch 310\tLoss value: 0.0178 ± 0.0031\n",
      "INFO:__main__:Epoch 320\tLoss value: 0.0168 ± 0.0038\n",
      "INFO:__main__:Epoch 330\tLoss value: 0.0161 ± 0.0037\n",
      "INFO:__main__:Epoch 340\tLoss value: 0.0154 ± 0.0036\n",
      "INFO:__main__:Epoch 350\tLoss value: 0.0169 ± 0.0032\n",
      "INFO:__main__:Epoch 360\tLoss value: 0.0162 ± 0.0027\n",
      "INFO:__main__:Epoch 370\tLoss value: 0.0156 ± 0.0029\n",
      "INFO:__main__:Epoch 380\tLoss value: 0.0167 ± 0.0034\n",
      "INFO:__main__:Epoch 390\tLoss value: 0.0156 ± 0.0033\n",
      "INFO:__main__:Epoch 400\tLoss value: 0.0173 ± 0.0031\n",
      "INFO:__main__:Epoch 410\tLoss value: 0.0148 ± 0.0031\n",
      "INFO:__main__:Epoch 420\tLoss value: 0.0155 ± 0.0031\n",
      "INFO:__main__:Epoch 430\tLoss value: 0.0165 ± 0.0029\n",
      "INFO:__main__:Epoch 440\tLoss value: 0.0159 ± 0.0030\n",
      "INFO:__main__:Epoch 450\tLoss value: 0.0163 ± 0.0023\n",
      "INFO:__main__:Epoch 460\tLoss value: 0.0166 ± 0.0038\n",
      "INFO:__main__:Epoch 470\tLoss value: 0.0144 ± 0.0022\n",
      "INFO:__main__:Epoch 480\tLoss value: 0.0160 ± 0.0040\n",
      "INFO:__main__:Epoch 490\tLoss value: 0.0151 ± 0.0036\n",
      "INFO:__main__:Epoch 500\tLoss value: 0.0139 ± 0.0027\n",
      "INFO:__main__:Epoch 510\tLoss value: 0.0160 ± 0.0019\n",
      "INFO:__main__:Epoch 520\tLoss value: 0.0144 ± 0.0025\n",
      "INFO:__main__:Epoch 530\tLoss value: 0.0146 ± 0.0021\n",
      "INFO:__main__:Epoch 540\tLoss value: 0.0139 ± 0.0022\n",
      "INFO:__main__:Epoch 550\tLoss value: 0.0155 ± 0.0031\n",
      "INFO:__main__:Epoch 560\tLoss value: 0.0154 ± 0.0030\n",
      "INFO:__main__:Epoch 570\tLoss value: 0.0160 ± 0.0028\n",
      "INFO:__main__:Epoch 580\tLoss value: 0.0160 ± 0.0033\n",
      "INFO:__main__:Epoch 590\tLoss value: 0.0151 ± 0.0025\n",
      "INFO:__main__:Epoch 600\tLoss value: 0.0130 ± 0.0036\n",
      "INFO:__main__:Epoch 610\tLoss value: 0.0158 ± 0.0026\n",
      "INFO:__main__:Epoch 620\tLoss value: 0.0134 ± 0.0025\n",
      "INFO:__main__:Epoch 630\tLoss value: 0.0152 ± 0.0029\n",
      "INFO:__main__:Epoch 640\tLoss value: 0.0128 ± 0.0033\n",
      "INFO:__main__:Epoch 650\tLoss value: 0.0145 ± 0.0026\n",
      "INFO:__main__:Epoch 660\tLoss value: 0.0138 ± 0.0027\n",
      "INFO:__main__:Epoch 670\tLoss value: 0.0141 ± 0.0022\n",
      "INFO:__main__:Epoch 680\tLoss value: 0.0144 ± 0.0021\n",
      "INFO:__main__:Epoch 690\tLoss value: 0.0134 ± 0.0017\n",
      "INFO:__main__:Epoch 700\tLoss value: 0.0127 ± 0.0027\n",
      "INFO:__main__:Epoch 710\tLoss value: 0.0148 ± 0.0027\n",
      "INFO:__main__:Epoch 720\tLoss value: 0.0140 ± 0.0022\n",
      "INFO:__main__:Epoch 730\tLoss value: 0.0145 ± 0.0027\n",
      "INFO:__main__:Epoch 740\tLoss value: 0.0135 ± 0.0031\n",
      "INFO:__main__:Epoch 750\tLoss value: 0.0127 ± 0.0034\n",
      "INFO:__main__:Epoch 760\tLoss value: 0.0161 ± 0.0049\n",
      "INFO:__main__:Epoch 770\tLoss value: 0.0125 ± 0.0035\n",
      "INFO:__main__:Epoch 780\tLoss value: 0.0130 ± 0.0024\n",
      "INFO:__main__:Epoch 790\tLoss value: 0.0150 ± 0.0027\n",
      "INFO:__main__:Epoch 800\tLoss value: 0.0144 ± 0.0035\n",
      "INFO:__main__:Epoch 810\tLoss value: 0.0144 ± 0.0024\n",
      "INFO:__main__:Epoch 820\tLoss value: 0.0133 ± 0.0028\n",
      "INFO:__main__:Epoch 830\tLoss value: 0.0148 ± 0.0015\n",
      "INFO:__main__:Epoch 840\tLoss value: 0.0147 ± 0.0033\n",
      "INFO:__main__:Epoch 850\tLoss value: 0.0132 ± 0.0024\n",
      "INFO:__main__:Epoch 860\tLoss value: 0.0130 ± 0.0024\n",
      "INFO:__main__:Epoch 870\tLoss value: 0.0135 ± 0.0027\n",
      "INFO:__main__:Epoch 880\tLoss value: 0.0147 ± 0.0013\n",
      "INFO:__main__:Epoch 890\tLoss value: 0.0162 ± 0.0038\n",
      "INFO:__main__:Epoch 900\tLoss value: 0.0129 ± 0.0037\n",
      "INFO:__main__:Epoch 910\tLoss value: 0.0131 ± 0.0022\n",
      "INFO:__main__:Epoch 920\tLoss value: 0.0134 ± 0.0017\n",
      "INFO:__main__:Epoch 930\tLoss value: 0.0127 ± 0.0021\n",
      "INFO:__main__:Epoch 940\tLoss value: 0.0127 ± 0.0032\n",
      "INFO:__main__:Epoch 950\tLoss value: 0.0141 ± 0.0023\n",
      "INFO:__main__:Epoch 960\tLoss value: 0.0123 ± 0.0033\n",
      "INFO:__main__:Epoch 970\tLoss value: 0.0133 ± 0.0026\n",
      "INFO:__main__:Epoch 980\tLoss value: 0.0121 ± 0.0024\n",
      "INFO:__main__:Epoch 990\tLoss value: 0.0152 ± 0.0016\n",
      "INFO:__main__:Epoch 1000\tLoss value: 0.0130 ± 0.0023\n"
     ]
    }
   ],
   "source": [
    "def stats(values):\n",
    "    return '{0:.4f} ± {1:.4f}'.format(round(np.mean(values), 4), round(np.std(values), 4))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_op)\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    order = random_state.permutation(nb_examples)\n",
    "    Xs_shuf, Xp_shuf, Xo_shuf = Xs[order], Xp[order], Xo[order]\n",
    "    \n",
    "    loss_values = []\n",
    "\n",
    "    for batch_no, (batch_start, batch_end) in enumerate(batches):\n",
    "        curr_batch_size = batch_end - batch_start\n",
    "\n",
    "        Xs_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xs_shuf.dtype)\n",
    "        Xp_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xp_shuf.dtype)\n",
    "        Xo_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xo_shuf.dtype)\n",
    "\n",
    "        Xs_batch[0::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "        Xp_batch[0::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[0::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "        # Xs_batch[1::nb_versions] needs to be corrupted\n",
    "        Xs_batch[1::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "        Xp_batch[1::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[1::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "        # Xo_batch[2::nb_versions] needs to be corrupted\n",
    "        Xs_batch[2::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "        Xp_batch[2::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[2::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "\n",
    "        feed_dict = {\n",
    "            subject_inputs: Xs_batch, predicate_inputs: Xp_batch, object_inputs: Xo_batch,\n",
    "            target_inputs: np.array([1.0, 0.0, 0.0] * curr_batch_size)\n",
    "        }\n",
    "\n",
    "        _, loss_value = session.run([training_step, loss], feed_dict=feed_dict)\n",
    "        session.run(projection_step)\n",
    "\n",
    "        loss_values += [loss_value / (Xp_batch.shape[0] / nb_versions)]\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        logger.info('Epoch {0}\\tLoss value: {1}'.format(epoch, stats(loss_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:0/5000 ..\n",
      "INFO:__main__:1000/5000 ..\n",
      "INFO:__main__:2000/5000 ..\n",
      "INFO:__main__:3000/5000 ..\n",
      "INFO:__main__:4000/5000 ..\n",
      "INFO:__main__:[valid] Raw Mean Rank: 822.6784\n",
      "INFO:__main__:[valid] Raw Hits@1: 41.349999999999994\n",
      "INFO:__main__:[valid] Raw Hits@3: 66.85\n",
      "INFO:__main__:[valid] Raw Hits@5: 75.14\n",
      "INFO:__main__:[valid] Raw Hits@10: 81.97\n",
      "INFO:__main__:[valid] Filtered Mean Rank: 812.4296\n",
      "INFO:__main__:[valid] Filtered Hits@1: 66.9\n",
      "INFO:__main__:[valid] Filtered Hits@3: 90.64999999999999\n",
      "INFO:__main__:[valid] Filtered Hits@5: 93.0\n",
      "INFO:__main__:[valid] Filtered Hits@10: 94.07\n",
      "INFO:__main__:0/5000 ..\n",
      "INFO:__main__:1000/5000 ..\n",
      "INFO:__main__:2000/5000 ..\n",
      "INFO:__main__:3000/5000 ..\n",
      "INFO:__main__:4000/5000 ..\n",
      "INFO:__main__:[test] Raw Mean Rank: 857.8714\n",
      "INFO:__main__:[test] Raw Hits@1: 41.06\n",
      "INFO:__main__:[test] Raw Hits@3: 67.44\n",
      "INFO:__main__:[test] Raw Hits@5: 75.12\n",
      "INFO:__main__:[test] Raw Hits@10: 82.43\n",
      "INFO:__main__:[test] Filtered Mean Rank: 847.4885\n",
      "INFO:__main__:[test] Filtered Hits@1: 66.09\n",
      "INFO:__main__:[test] Filtered Hits@3: 90.67\n",
      "INFO:__main__:[test] Filtered Hits@5: 93.10000000000001\n",
      "INFO:__main__:[test] Filtered Hits@10: 94.19999999999999\n"
     ]
    }
   ],
   "source": [
    "for eval_name, eval_triples in [('valid', valid_triples), ('test', test_triples)]:\n",
    "\n",
    "    ranks_subj, ranks_obj = [], []\n",
    "    filtered_ranks_subj, filtered_ranks_obj = [], []\n",
    "\n",
    "    for _i, (s, p, o) in enumerate(eval_triples):\n",
    "        s_idx, p_idx, o_idx = entity_to_idx[s], predicate_to_idx[p], entity_to_idx[o]\n",
    "\n",
    "        Xs = np.full(shape=(nb_entities,), fill_value=s_idx, dtype=np.int32)\n",
    "        Xp = np.full(shape=(nb_entities,), fill_value=p_idx, dtype=np.int32)\n",
    "        Xo = np.full(shape=(nb_entities,), fill_value=o_idx, dtype=np.int32)\n",
    "\n",
    "        feed_dict_corrupt_subj = {subject_inputs: np.arange(nb_entities), predicate_inputs: Xp, object_inputs: Xo}\n",
    "        feed_dict_corrupt_obj = {subject_inputs: Xs, predicate_inputs: Xp, object_inputs: np.arange(nb_entities)}\n",
    "\n",
    "        # scores of (1, p, o), (2, p, o), .., (N, p, o)\n",
    "        scores_subj = session.run(scores, feed_dict=feed_dict_corrupt_subj)\n",
    "\n",
    "        # scores of (s, p, 1), (s, p, 2), .., (s, p, N)\n",
    "        scores_obj = session.run(scores, feed_dict=feed_dict_corrupt_obj)\n",
    "\n",
    "        ranks_subj += [1 + np.sum(scores_subj > scores_subj[s_idx])]\n",
    "        ranks_obj += [1 + np.sum(scores_obj > scores_obj[o_idx])]\n",
    "\n",
    "        filtered_scores_subj = scores_subj.copy()\n",
    "        filtered_scores_obj = scores_obj.copy()\n",
    "\n",
    "        rm_idx_s = [entity_to_idx[fs] for (fs, fp, fo) in all_triples if fs != s and fp == p and fo == o]\n",
    "        rm_idx_o = [entity_to_idx[fo] for (fs, fp, fo) in all_triples if fs == s and fp == p and fo != o]\n",
    "\n",
    "        filtered_scores_subj[rm_idx_s] = - np.inf\n",
    "        filtered_scores_obj[rm_idx_o] = - np.inf\n",
    "\n",
    "        filtered_ranks_subj += [1 + np.sum(filtered_scores_subj > filtered_scores_subj[s_idx])]\n",
    "        filtered_ranks_obj += [1 + np.sum(filtered_scores_obj > filtered_scores_obj[o_idx])]\n",
    "\n",
    "        if _i % 1000 == 0:\n",
    "            logger.info('{}/{} ..'.format(_i, len(eval_triples)))\n",
    "        \n",
    "        \n",
    "    ranks = ranks_subj + ranks_obj\n",
    "    filtered_ranks = filtered_ranks_subj + filtered_ranks_obj\n",
    "\n",
    "    for setting_name, setting_ranks in [('Raw', ranks), ('Filtered', filtered_ranks)]:\n",
    "        mean_rank = np.mean(setting_ranks)\n",
    "        logger.info('[{}] {} Mean Rank: {}'.format(eval_name, setting_name, mean_rank))\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n",
    "            logger.info('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
